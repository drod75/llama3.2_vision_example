<div align='center'>
    <img src='https://ollama.com/public/ollama.png' alt='ollama' width=300>
    <h1>Ollama 3.2 Vision Demonstration</h1>
</div> <br>

<details open>
<summary>Table of Contents</summary>
<ol>
  <li>
    <a href="#introduction">Introduction</a>
  </li>
  <li>
    <a href="#tools-used">Tools Used</a>
  </li>
  <li>
    <a href="#demonstration">Demonstration</a>
  </li>
  <li>
    <a href="#future">Future</a>
  </li>
</ol>
</details>

## Introduction
This is a repository made to demonstrate the cability and usefulness of Llama 3.2 vision, it is meant to show how the model can do in a professional use case, an example of how
it should do is on [Blind-Spot](https://www.blind-spot.app) which uses Google Gemini. The whole point is to test whether or not this model can act as a good replacement and if it exceeds the performance and usefulness of Gemini, the next phase of testing will occur after work in this repository is done which will be the use of an agent through one of the models or both, to test which is better and should be used in the for seeable future.
<br>


## Tools Used
The tools used are:
<!-- badges -->
[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=yellow)](https://www.python.org/)
[![Ollama](https://img.shields.io/badge/Ollama-%23000000?style=for-the-badge&logo=ollama&logoColor=white)](https://ollama.com/)
[![Langchain](https://img.shields.io/badge/Langchain-%2300A9FF?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyBmaWxsPSIjMDM2QkZGIiByb2xlPSJpbWciIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48dGl0bGU+TGFuZ0NoYWluPC90aXRsZT48cGF0aCBkPSJNMTUuOTggMi4wMDFIMTRjLS45NyAwLTEuNjYtMS4wMy0yLTIuMDAxaDJjLjM0IDEuNSAxIDEuOTk3IDIgMi4wMDF2MmgtMmMtMS4xIDAtMiAuOS0yIDJ2Mi4wNzZjMCAxLjA5OC0uODcgMS45NzMtMiAxLjk3M0g0Yy0xLjEgMC0yIC45LTIgMnY0YzAgMS4xLjkgMiAyIDJoMy45OTljMS4wMDQgMCAxLjkzLTEuMDc4IDIuMDAxLTJIOVYxNGMwLTEuMS45LTIgMi0yaDIuMDI4Yy4wMDEtMS4xLjkxNy0yLjAwMyAyLjAwNi0yLjAwM0gxOFY4YzAtMS4xLS45LTItMi0yaC0yLjAyOGMuMDQ3LTEuMDQtLjg1My0yLTIuMDAyLTJ6TTggNC4wMDFoLTIuMDI4Yy4wNDctMS4wNC0uODUzLTItMi4wMDItMkg2djJoMi4wMmMxLjEzMyAwIDEuOTc5Ljg3IDEuOTggMnptMTIgMTZINHYtNGgydi0yaC4wMDFjMS4xOTIgMCAyLjA3LS44NzkgMi4wNy0yVjkuOTk3SDROdjJoNnYyaDEuOTk5QzExLjkzIDE2LjA3OCA5IDE5LjAxMiA5IDE5LjAxMmgtM3YyLjAwMWgzLjc1N2MuMi41LjQ3OCAxLjA0My43NzggMS40MDJsLjE0LjE4MmgyLjM5NWMxLjU5NSAwIDIuMDQ4LS45OTYgMi40MDUtMi4wMDFIMTh2LTJoLTQuMDMyYy0uMzc0LTEuMDUxLTEuNDc4LTIuNDU0LTEuNDc4LTIuNDU0bDQuNDg4IDBoMi4wMjN2NC4wMDF6Ij48L3BhdGg+PC9zdmc+&logoColor=white)](https://www.langchain.com/)
[![Streamlit](https://img.shields.io/badge/Streamlit-%23FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)](https://streamlit.io/)
<br>


## Demonstration
The way the image description will be demonstrated is via a video on a basic Streamlit Application that will use the model to take in various images and show the description out to the user, the whole point is to mimic [Blind-Spot](https://www.blind-spot.app)s and show if any changes can be made to the model to make better descriptions and help blind people better through the use of a new model.
<a href='https://www.youtube.com/watch?v=dQw4w9WgXcQ&pp=0gcJCdgAo7VqN5tD'>Video Link TBD</a>
<br>


## Future
As stated before the point is to create an agent using either Gemini or Ollama to provide better descriptions to the user while ensuring the best possible resources and model. Any future repository made based on this data will be listed below.
<br>